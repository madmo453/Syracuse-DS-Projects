{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Michael Morales - IST 664 Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import pandas\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.corpus import sentence_polarity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus.reader.api import *\n",
    "from nltk.tokenize import *\n",
    "from nltk.collocations import *\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sentiment_read_LIWC_pos_neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory path\n",
    "\n",
    "dirPath = 'C:/Users/madmo/OneDrive/Syracuse/IST664 - NLP/Final Project/kagglemoviereviews'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: define features\n",
    "\n",
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: cross-validation\n",
    "\n",
    "def cross_validation_PRF(num_folds, featuresets, labels):\n",
    "    subset_size = int(len(featuresets)/num_folds)\n",
    "    print('Each fold size:', subset_size)\n",
    "    # for the number of labels - start the totals lists with zeroes\n",
    "    num_labels = len(labels)\n",
    "    total_precision_list = [0] * num_labels\n",
    "    total_recall_list = [0] * num_labels\n",
    "    total_F1_list = [0] * num_labels\n",
    "\n",
    "    # iterate over the folds\n",
    "    for i in range(num_folds):\n",
    "        test_this_round = featuresets[(i*subset_size):][:subset_size]\n",
    "        train_this_round = featuresets[:(i*subset_size)] + featuresets[((i+1)*subset_size):]\n",
    "        # train using train_this_round\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train_this_round)\n",
    "        # evaluate against test_this_round to produce the gold and predicted labels\n",
    "        goldlist = []\n",
    "        predictedlist = []\n",
    "        for (features, label) in test_this_round:\n",
    "            goldlist.append(label)\n",
    "            predictedlist.append(classifier.classify(features))\n",
    "\n",
    "        # computes evaluation measures for this fold and\n",
    "        #   returns list of measures for each label\n",
    "        print('Fold', i)\n",
    "        (precision_list, recall_list, F1_list) \\\n",
    "                  = eval_measures(goldlist, predictedlist, labels)\n",
    "        # take off triple string to print precision, recall and F1 for each fold\n",
    "        '''\n",
    "        print('\\tPrecision\\tRecall\\t\\tF1')\n",
    "        # print measures for each label\n",
    "        for i, lab in enumerate(labels):\n",
    "            print(lab, '\\t', \"{:10.3f}\".format(precision_list[i]), \\\n",
    "              \"{:10.3f}\".format(recall_list[i]), \"{:10.3f}\".format(F1_list[i]))\n",
    "        '''\n",
    "        # for each label add to the sums in the total lists\n",
    "        for i in range(num_labels):\n",
    "            # for each label, add the 3 measures to the 3 lists of totals\n",
    "            total_precision_list[i] += precision_list[i]\n",
    "            total_recall_list[i] += recall_list[i]\n",
    "            total_F1_list[i] += F1_list[i]\n",
    "\n",
    "    # find precision, recall and F measure averaged over all rounds for all labels\n",
    "    # compute averages from the totals lists\n",
    "    precision_list = [tot/num_folds for tot in total_precision_list]\n",
    "    recall_list = [tot/num_folds for tot in total_recall_list]\n",
    "    F1_list = [tot/num_folds for tot in total_F1_list]\n",
    "    # the evaluation measures in a table with one row per label\n",
    "    print('\\nAverage Precision\\tRecall\\t\\tF1 \\tPer Label')\n",
    "    # print measures for each label\n",
    "    for i, lab in enumerate(labels):\n",
    "        print(lab, '\\t', \"{:10.3f}\".format(precision_list[i]), \\\n",
    "          \"{:10.3f}\".format(recall_list[i]), \"{:10.3f}\".format(F1_list[i]))\n",
    "    \n",
    "    # print macro average over all labels - treats each label equally\n",
    "    print('\\nMacro Average Precision\\tRecall\\t\\tF1 \\tOver All Labels')\n",
    "    print('\\t', \"{:10.3f}\".format(sum(precision_list)/num_labels), \\\n",
    "          \"{:10.3f}\".format(sum(recall_list)/num_labels), \\\n",
    "          \"{:10.3f}\".format(sum(F1_list)/num_labels))\n",
    "\n",
    "    # for micro averaging, weight the scores for each label by the number of items\n",
    "    #    this is better for labels with imbalance\n",
    "    # first intialize a dictionary for label counts and then count them\n",
    "    label_counts = {}\n",
    "    for lab in labels:\n",
    "      label_counts[lab] = 0 \n",
    "    # count the labels\n",
    "    for (doc, lab) in featuresets:\n",
    "      label_counts[lab] += 1\n",
    "    # make weights compared to the number of documents in featuresets\n",
    "    num_docs = len(featuresets)\n",
    "    label_weights = [(label_counts[lab] / num_docs) for lab in labels]\n",
    "    print('\\nLabel Counts', label_counts)\n",
    "    #print('Label weights', label_weights)\n",
    "    # print macro average over all labels\n",
    "    print('Micro Average Precision\\tRecall\\t\\tF1 \\tOver All Labels')\n",
    "    precision = sum([a * b for a,b in zip(precision_list, label_weights)])\n",
    "    recall = sum([a * b for a,b in zip(recall_list, label_weights)])\n",
    "    F1 = sum([a * b for a,b in zip(F1_list, label_weights)])\n",
    "    print( '\\t', \"{:10.3f}\".format(precision), \\\n",
    "      \"{:10.3f}\".format(recall), \"{:10.3f}\".format(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: compute precision, recall, and F1\n",
    "\n",
    "def eval_measures(gold, predicted, labels):\n",
    "    \n",
    "    # these lists have values for each label \n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    F1_list = []\n",
    "\n",
    "    for lab in labels:\n",
    "        # for each label, compare gold and predicted lists and compute values\n",
    "        TP = FP = FN = TN = 0\n",
    "        for i, val in enumerate(gold):\n",
    "            if val == lab and predicted[i] == lab:  TP += 1\n",
    "            if val == lab and predicted[i] != lab:  FN += 1\n",
    "            if val != lab and predicted[i] == lab:  FP += 1\n",
    "            if val != lab and predicted[i] != lab:  TN += 1\n",
    "        # use these to compute recall, precision, F1\n",
    "        # for small numbers, guard against dividing by zero in computing measures\n",
    "        if (TP == 0) or (FP == 0) or (FN == 0):\n",
    "          recall_list.append (0)\n",
    "          precision_list.append (0)\n",
    "          F1_list.append(0)\n",
    "        else:\n",
    "          recall = TP / (TP + FP)\n",
    "          precision = TP / (TP + FN)\n",
    "          recall_list.append(recall)\n",
    "          precision_list.append(precision)\n",
    "          F1_list.append( 2 * (recall * precision) / (recall + precision))\n",
    "\n",
    "    # the evaluation measures in a table with one row per label\n",
    "    return (precision_list, recall_list, F1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: define bigram features\n",
    "\n",
    "def bigram_document_features(document, word_features, bigram_features):\n",
    "    document_words = set(document)\n",
    "    document_bigrams = nltk.bigrams(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    for bigram in bigram_features:\n",
    "        features['B_{}_{}'.format(bigram[0], bigram[1])] = (bigram in document_bigrams)    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: write featuresets to CSV file\n",
    "\n",
    "def writeFeatureSets(featuresets, outpath):\n",
    "    # Open outpath for writing\n",
    "    f = open(outpath, 'w')\n",
    "    # Get the feature names from the feature dictionary in the first featureset\n",
    "    featurenames = featuresets[0][0].keys()\n",
    "    # Create the first list of the file as comma-separated feature names\n",
    "    #   with the word class as the last feature name\n",
    "    featurenameline = ''\n",
    "    for featurename in featurenames:\n",
    "        # Replace forbidden characters with text abbreviations\n",
    "        featurename = featurename.replace(',','CM')\n",
    "        featurename = featurename.replace(\"'\",\"DQ\")\n",
    "        featurename = featurename.replace('\"','QU')\n",
    "        featurenameline += featurename + ','\n",
    "        featurenameline += 'class'\n",
    "    # Write this as the first line in the csv file\n",
    "    f.write(featurenameline)\n",
    "    f.write('\\n')\n",
    "    # Convert each feature set to a line in the file with comma separated feature values,\n",
    "    # each feature value is converted to a string \n",
    "    #   for booleans this is the words true and false\n",
    "    #   for numbers, this is the string with the number\n",
    "    for featureset in featuresets:\n",
    "        featureline = ''\n",
    "        for key in featurenames:\n",
    "            try:\n",
    "                featureline += str(featureset[0].get(key, []))+','\n",
    "            except KeyError:\n",
    "                continue\n",
    "        featureline += str(featureset[1])\n",
    "        # Write each feature set values to the file\n",
    "        f.write(featureline)\n",
    "        f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: define sentiment lexicon features\n",
    "\n",
    "def readSubjectivity(path):\n",
    "\tflexicon = open(path, 'r')\n",
    "\t# initialize an empty dictionary\n",
    "\tsldict = { }\n",
    "\tfor line in flexicon:\n",
    "\t\tfields = line.split()   # default is to split on whitespace\n",
    "\t\t# split each field on the '=' and keep the second part as the value\n",
    "\t\tstrength = fields[0].split(\"=\")[1]\n",
    "\t\tword = fields[2].split(\"=\")[1]\n",
    "\t\tposTag = fields[3].split(\"=\")[1]\n",
    "\t\tstemmed = fields[4].split(\"=\")[1]\n",
    "\t\tpolarity = fields[5].split(\"=\")[1]\n",
    "\t\tif (stemmed == 'y'):\n",
    "\t\t\tisStemmed = True\n",
    "\t\telse:\n",
    "\t\t\tisStemmed = False\n",
    "\t\t# put a dictionary entry with the word as the keyword\n",
    "\t\t#     and a list of the other values\n",
    "\t\tsldict[word] = [strength, posTag, isStemmed, polarity]\n",
    "\treturn sldict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: define sentiment lexicon features\n",
    "\n",
    "def SL_features(document, word_features, SL):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    # count variables for the 4 classes of subjectivity\n",
    "    weakPos = 0\n",
    "    strongPos = 0\n",
    "    weakNeg = 0\n",
    "    strongNeg = 0\n",
    "    for word in document_words:\n",
    "        if word in SL:\n",
    "            strength, posTag, isStemmed, polarity = SL[word]\n",
    "            if strength == 'weaksubj' and polarity == 'positive':\n",
    "                weakPos += 1\n",
    "            if strength == 'strongsubj' and polarity == 'positive':\n",
    "                strongPos += 1\n",
    "            if strength == 'weaksubj' and polarity == 'negative':\n",
    "                weakNeg += 1\n",
    "            if strength == 'strongsubj' and polarity == 'negative':\n",
    "                strongNeg += 1\n",
    "            features['positivecount'] = weakPos + (2 * strongPos)\n",
    "            features['negativecount'] = weakNeg + (2 * strongNeg)      \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: retrieve poslist and neglist from LIWC dictionary\n",
    "\n",
    "def read_words():\n",
    "  poslist = []\n",
    "  neglist = []\n",
    "\n",
    "  flexicon = open('C:/Users/madmo/OneDrive/Syracuse/IST664 - NLP/Final Project/liwcdic2007.dic', encoding='latin1')\n",
    "  # read all LIWC words from file\n",
    "  wordlines = [line.strip() for line in flexicon]\n",
    "  # each line has a word or a stem followed by * and numbers of the word classes it is in\n",
    "  # word class 126 is positive emotion and 127 is negative emotion\n",
    "  for line in wordlines:\n",
    "    if not line == '':\n",
    "      items = line.split()\n",
    "      word = items[0]\n",
    "      classes = items[1:]\n",
    "      for c in classes:\n",
    "        if c == '126':\n",
    "          poslist.append( word )\n",
    "        if c == '127':\n",
    "          neglist.append( word )\n",
    "  return (poslist, neglist)\n",
    "\n",
    "poslist, neglist = read_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: define LIWC sentiment lexicon features\n",
    "# https://github.com/bjprogrammer/Kaggle-Movie-Review/blob/master/kagglemoviereviews/classifyKaggle.py\n",
    "\n",
    "def liwc_features(doc, word_features,poslist,neglist):\n",
    "  doc_words = set(doc)\n",
    "  features = {}\n",
    "  for word in word_features:\n",
    "    features['contains({})'.format(word)] = (word in doc_words)\n",
    "  pos = 0\n",
    "  neg = 0\n",
    "  for word in doc_words:\n",
    "    if sentiment_read_LIWC_pos_neg_words.isPresent(word,poslist):\n",
    "      pos += 1\n",
    "    if sentiment_read_LIWC_pos_neg_words.isPresent(word,neglist):\n",
    "      neg += 1\n",
    "    features['positivecount'] = pos\n",
    "    features['negativecount'] = neg\n",
    "  if 'positivecount' not in features:\n",
    "    features['positivecount']=0\n",
    "  if 'negativecount' not in features:\n",
    "    features['negativecount']=0  \n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: define a combination of SL and LIWC lexicons\n",
    "# https://github.com/bjprogrammer/Kaggle-Movie-Review/blob/master/kagglemoviereviews/classifyKaggle.py\n",
    "\n",
    "def SL_liwc_features(doc, word_features, SL,poslist,neglist):\n",
    "  document_words = set(doc)\n",
    "  features = {}\n",
    "  for word in word_features:\n",
    "    features['contains({})'.format(word)] = (word in document_words)\n",
    "  # count variables for the 4 classes of subjectivity\n",
    "  weakPos = 0\n",
    "  strongPos = 0\n",
    "  weakNeg = 0\n",
    "  strongNeg = 0\n",
    "  for word in document_words:\n",
    "    if sentiment_read_LIWC_pos_neg_words.isPresent(word,poslist):\n",
    "      strongPos += 1\n",
    "    elif sentiment_read_LIWC_pos_neg_words.isPresent(word,neglist):\n",
    "      strongNeg += 1\n",
    "    elif word in SL:\n",
    "      strength, posTag, isStemmed, polarity = SL[word]\n",
    "      if strength == 'weaksubj' and polarity == 'positive':\n",
    "        weakPos += 1\n",
    "      if strength == 'strongsubj' and polarity == 'positive':\n",
    "        strongPos += 1\n",
    "      if strength == 'weaksubj' and polarity == 'negative':\n",
    "        weakNeg += 1\n",
    "      if strength == 'strongsubj' and polarity == 'negative':\n",
    "        strongNeg += 1\n",
    "    features['positivecount'] = weakPos + (2 * strongPos)\n",
    "    features['negativecount'] = weakNeg + (2 * strongNeg)\n",
    "  \n",
    "  if 'positivecount' not in features:\n",
    "    features['positivecount']=0\n",
    "  if 'negativecount' not in features:\n",
    "    features['negativecount']=0      \n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: define part-of-speech tagging features\n",
    "\n",
    "def POS_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    tagged_words = nltk.pos_tag(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    numNoun = 0\n",
    "    numVerb = 0\n",
    "    numAdj = 0\n",
    "    numAdverb = 0\n",
    "    for (word, tag) in tagged_words:\n",
    "        if tag.startswith('N'): numNoun += 1\n",
    "        if tag.startswith('V'): numVerb += 1\n",
    "        if tag.startswith('J'): numAdj += 1\n",
    "        if tag.startswith('R'): numAdverb += 1\n",
    "    features['nouns'] = numNoun\n",
    "    features['verbs'] = numVerb\n",
    "    features['adjectives'] = numAdj\n",
    "    features['adverbs'] = numAdverb\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Representing negation\n",
    "\n",
    "def NOT_features(document, word_features, negationwords):\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = False\n",
    "        features['V_NOT{}'.format(word)] = False\n",
    "    # go through document words in order\n",
    "    for i in range(0, len(document)):\n",
    "        word = document[i]\n",
    "        if ((i + 1) < len(document)) and ((word in negationwords) or (word.endswith(\"n't\"))):\n",
    "            i += 1\n",
    "            features['V_NOT{}'.format(document[i])] = (document[i] in word_features)\n",
    "        else:\n",
    "            features['V_{}'.format(word)] = (word in word_features)\n",
    "    return features\n",
    "\n",
    "negationwords = ['no', 'not', 'never', 'none', 'nowhere', 'nothing', 'noone', 'rather', 'hardly', 'scarcely', 'rarely', 'seldom', 'neither', 'nor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Bing Liu's Opinion Lexicon\n",
    "\n",
    "dirPath = 'C:/Users/madmo/OneDrive/Syracuse/IST664 - NLP/Final Project/kagglemoviereviews'\n",
    "\n",
    "def read_opinionlexicon():\n",
    "    POSITIVE_REVIEWS = 'C:/Users/madmo/OneDrive/Syracuse/IST664 - NLP/Final Project/rt-polarity-pos.txt'\n",
    "    NEGATIVE_REVIEWS = 'C:/Users/madmo/OneDrive/Syracuse/IST664 - NLP/Final Project/rt-polarity-neg.txt'\n",
    "    \n",
    "    pos_features = []\n",
    "    neg_features = []\n",
    "    for line in open(POSITIVE_REVIEWS, 'r').readlines()[35:]:\n",
    "        pos_words = re.findall(r\"[\\w']+|[.,!?;]\", line.rstrip())\n",
    "        pos_features.append(pos_words[0])\n",
    "        \n",
    "    for line in open(NEGATIVE_REVIEWS, 'r').readlines()[35:]:\n",
    "        neg_words = re.findall(r\"[\\w']+|[.,!?;]\", line.rstrip())\n",
    "        neg_features.append(neg_words[0])\n",
    "  \n",
    "    return pos_features,neg_features\n",
    "\n",
    "poslist2,neglist2 = read_opinionlexicon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10357\n"
     ]
    }
   ],
   "source": [
    "# I could not get this code to work when encapsulated in the processkaggle() function\n",
    "# Featureset1: Bag of words / unigram (baseline)\n",
    "\n",
    "vocab_size = 500\n",
    "limit = int(10000)\n",
    "os.chdir(dirPath)\n",
    "f = open('./train.tsv', 'r')\n",
    "phrasedata=[]\n",
    "for line in f:\n",
    "    if (not line.startswith('Phrase')):\n",
    "        line = line.strip()\n",
    "        phrasedata.append(line.split('\\t')[2:4])\n",
    "random.shuffle(phrasedata)\n",
    "phraselist = phrasedata[:limit]\n",
    "phrasedocs = []\n",
    "for phrase in phraselist:\n",
    "    tokens = nltk.word_tokenize(phrase[0])\n",
    "    phrasedocs.append((tokens, int(phrase[1])))\n",
    "docs = []\n",
    "for phrase in phrasedocs:\n",
    "    lowerphrase = ([w.lower() for w in phrase[0]], phrase[1])\n",
    "    docs.append(lowerphrase)\n",
    "all_words_list = [word for (sent, cat) in docs for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "print(len(all_words))\n",
    "word_items = all_words.most_common(vocab_size)\n",
    "word_features = [word for (word, count) in word_items]\n",
    "featuresets = [(document_features(d, word_features), c) for (d,c) in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [c for (d,c) in docs]\n",
    "labels = list(set(label_list))\n",
    "num_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.208      0.178      0.190\n",
      "1 \t      0.214      0.344      0.263\n",
      "2 \t      0.826      0.620      0.709\n",
      "3 \t      0.215      0.390      0.277\n",
      "4 \t      0.179      0.277      0.217\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.328      0.362      0.331\n",
      "\n",
      "Label Counts {0: 444, 1: 1691, 2: 5160, 3: 2119, 4: 586}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.528      0.485      0.490\n",
      "39.65596914291382  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featuresets, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.504"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featuresets(baseline)\n",
    "\n",
    "train_set, test_set = featuresets[1000:], featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.208      0.177      0.190\n",
      "1 \t      0.214      0.344      0.263\n",
      "2 \t      0.827      0.620      0.709\n",
      "3 \t      0.215      0.390      0.277\n",
      "4 \t      0.179      0.277      0.217\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.328      0.362      0.331\n",
      "\n",
      "Label Counts {0: 444, 1: 1691, 2: 5160, 3: 2119, 4: 586}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.528      0.485      0.490\n",
      "79.61417579650879  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset2: bigram\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(all_words_list)\n",
    "bigram_features = finder.nbest(bigram_measures.chi_sq, 500)\n",
    "# print(bigram_features[:50])\n",
    "featuresets2 = [(bigram_document_features(d, word_features, bigram_features), c) for (d, c) in docs]\n",
    "\n",
    "# Cross-validation\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featuresets2, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.504"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featuresets2(bigrams)\n",
    "\n",
    "train_set, test_set = featuresets2[1000:], featuresets2[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.248      0.182      0.209\n",
      "1 \t      0.242      0.328      0.279\n",
      "2 \t      0.751      0.666      0.706\n",
      "3 \t      0.353      0.418      0.383\n",
      "4 \t      0.255      0.280      0.267\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.370      0.375      0.369\n",
      "\n",
      "Label Counts {0: 444, 1: 1691, 2: 5160, 3: 2119, 4: 586}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.529      0.512      0.517\n",
      "39.48142886161804  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset3: Sentiment Lexicon\n",
    "\n",
    "SLpath = 'C:/Users/madmo/OneDrive/Syracuse/IST664 - NLP/Final Project/kagglemoviereviews/SentimentLexicons/subjclueslen1-HLTEMNLP05.tff'\n",
    "SL = readSubjectivity(SLpath)\n",
    "\n",
    "featureset3 = [(SL_features(d, word_features, SL), c) for (d, c) in docs]\n",
    "\n",
    "# Cross-validation\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featureset3, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.487"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featureset3 (sentiment lexicon)\n",
    "\n",
    "train_set, test_set = featureset3[1000:], featureset3[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.232      0.195      0.211\n",
      "1 \t      0.213      0.349      0.264\n",
      "2 \t      0.817      0.639      0.717\n",
      "3 \t      0.276      0.424      0.334\n",
      "4 \t      0.227      0.293      0.255\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.353      0.380      0.356\n",
      "\n",
      "Label Counts {0: 444, 1: 1691, 2: 5160, 3: 2119, 4: 586}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.540      0.504      0.510\n",
      "39.65498113632202  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset4 : LIWC\n",
    "\n",
    "featureset4 = [(liwc_features(d, word_features,poslist,neglist), c) for (d,c) in docs]\n",
    "\n",
    "# Cross-validation\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featureset4, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.509"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featureset4 (LIWC sentiment lexicon)\n",
    "\n",
    "train_set, test_set = featureset4[1000:], featureset4[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.241      0.192      0.213\n",
      "1 \t      0.219      0.353      0.270\n",
      "2 \t      0.812      0.647      0.720\n",
      "3 \t      0.289      0.421      0.342\n",
      "4 \t      0.245      0.314      0.275\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.361      0.385      0.364\n",
      "\n",
      "Label Counts {0: 444, 1: 1691, 2: 5160, 3: 2119, 4: 586}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.542      0.509      0.515\n",
      "38.23581027984619  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset5: Combination SL and LIWC\n",
    "\n",
    "featureset5 = [(SL_liwc_features(d, word_features, SL, poslist, neglist), c) for (d,c) in docs]\n",
    "\n",
    "# Cross-validation\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featureset5, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.515"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featureset5 (combination SL and LIWC)\n",
    "\n",
    "train_set, test_set = featureset5[1000:], featureset5[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.247      0.154      0.189\n",
      "1 \t      0.234      0.344      0.279\n",
      "2 \t      0.803      0.630      0.706\n",
      "3 \t      0.198      0.381      0.260\n",
      "4 \t      0.208      0.272      0.235\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.338      0.356      0.334\n",
      "\n",
      "Label Counts {0: 444, 1: 1691, 2: 5160, 3: 2119, 4: 586}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.519      0.487      0.489\n",
      "39.859434604644775  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset 6: Part-of-speech tagging\n",
    "\n",
    "featureset6 = [(POS_features(d, word_features), c) for (d, c) in docs]\n",
    "\n",
    "# Cross-validation\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featureset6, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.495"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featureset6 (part-of-speech tagging)\n",
    "\n",
    "train_set, test_set = featureset6[1000:], featureset6[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.470      0.139      0.214\n",
      "1 \t      0.217      0.348      0.267\n",
      "2 \t      0.680      0.687      0.684\n",
      "3 \t      0.261      0.425      0.323\n",
      "4 \t      0.381      0.216      0.275\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.402      0.363      0.352\n",
      "\n",
      "Label Counts {0: 444, 1: 1691, 2: 5160, 3: 2119, 4: 586}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.486      0.522      0.492\n",
      "77.46689009666443  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset 7: Representing negation\n",
    "\n",
    "featureset7 = [(NOT_features(d, word_features, negationwords), c) for (d, c) in docs]\n",
    "\n",
    "# Cross-validation\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featureset7, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.474"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featureset7 (representing negation)\n",
    "\n",
    "train_set, test_set = featureset7[1000:], featureset7[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.225      0.141      0.172\n",
      "1 \t      0.230      0.324      0.269\n",
      "2 \t      0.790      0.626      0.698\n",
      "3 \t      0.205      0.378      0.266\n",
      "4 \t      0.197      0.272      0.228\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.329      0.348      0.327\n",
      "\n",
      "Label Counts {0: 444, 1: 1691, 2: 5160, 3: 2119, 4: 586}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.511      0.480      0.483\n",
      "37.89269280433655  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset 8: Using Bing Liu's Opinion Lexicon, obtained at:\n",
    "#   https://www.cs.uic.edu/~liub/\n",
    "\n",
    "featureset8 = [(liwc_features(d, word_features,poslist2,neglist2), c) for (d,c) in docs]\n",
    "\n",
    "# Cross-validation\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featureset8, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.483"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featureset8 (Bing Liu's Opinion Lexicon)\n",
    "\n",
    "train_set, test_set = featureset8[1000:], featureset8[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10366\n"
     ]
    }
   ],
   "source": [
    "# Featureset1: Bag of words / unigram (baseline)\n",
    "\n",
    "vocab_size = 1000\n",
    "limit = int(10000)\n",
    "os.chdir(dirPath)\n",
    "f = open('./train.tsv', 'r')\n",
    "phrasedata=[]\n",
    "for line in f:\n",
    "    if (not line.startswith('Phrase')):\n",
    "        line = line.strip()\n",
    "        phrasedata.append(line.split('\\t')[2:4])\n",
    "random.shuffle(phrasedata)\n",
    "phraselist = phrasedata[:limit]\n",
    "phrasedocs = []\n",
    "for phrase in phraselist:\n",
    "    tokens = nltk.word_tokenize(phrase[0])\n",
    "    phrasedocs.append((tokens, int(phrase[1])))\n",
    "docs = []\n",
    "for phrase in phrasedocs:\n",
    "    lowerphrase = ([w.lower() for w in phrase[0]], phrase[1])\n",
    "    docs.append(lowerphrase)\n",
    "all_words_list = [word for (sent, cat) in docs for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "print(len(all_words))\n",
    "word_items = all_words.most_common(vocab_size)\n",
    "word_features = [word for (word, count) in word_items]\n",
    "featuresets = [(document_features(d, word_features), c) for (d,c) in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.222      0.165      0.189\n",
      "1 \t      0.243      0.387      0.298\n",
      "2 \t      0.826      0.630      0.715\n",
      "3 \t      0.215      0.392      0.277\n",
      "4 \t      0.195      0.282      0.229\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.340      0.371      0.342\n",
      "\n",
      "Label Counts {0: 455, 1: 1745, 2: 5136, 3: 2026, 4: 638}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.533      0.496      0.499\n",
      "92.4079442024231  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset 1: Bag of words / unigram (baseline), vocabulary size 1000\n",
    "\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featuresets, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.528"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featuresets(baseline), vocab 1000\n",
    "\n",
    "train_set, test_set = featuresets[1000:], featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.222      0.165      0.189\n",
      "1 \t      0.243      0.387      0.298\n",
      "2 \t      0.826      0.630      0.715\n",
      "3 \t      0.215      0.392      0.277\n",
      "4 \t      0.195      0.282      0.229\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.340      0.371      0.342\n",
      "\n",
      "Label Counts {0: 455, 1: 1745, 2: 5136, 3: 2026, 4: 638}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.533      0.496      0.499\n",
      "129.47090649604797  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset2: bigram, vocab 1000\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(all_words_list)\n",
    "bigram_features = finder.nbest(bigram_measures.chi_sq, 500)\n",
    "# print(bigram_features[:50])\n",
    "featuresets2 = [(bigram_document_features(d, word_features, bigram_features), c) for (d, c) in docs]\n",
    "\n",
    "# Cross-validation\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featuresets2, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.528"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featuresets2(bigrams), vocab 1000\n",
    "\n",
    "train_set, test_set = featuresets2[1000:], featuresets2[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.270      0.191      0.223\n",
      "1 \t      0.297      0.400      0.340\n",
      "2 \t      0.764      0.674      0.716\n",
      "3 \t      0.345      0.423      0.379\n",
      "4 \t      0.288      0.309      0.296\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.393      0.399      0.391\n",
      "\n",
      "Label Counts {0: 455, 1: 1745, 2: 5136, 3: 2026, 4: 638}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.545      0.530      0.533\n",
      "81.77736592292786  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset3: Sentiment Lexicon, vocab 1000\n",
    "\n",
    "SLpath = 'C:/Users/madmo/OneDrive/Syracuse/IST664 - NLP/Final Project/kagglemoviereviews/kagglemoviereviews/SentimentLexicons/subjclueslen1-HLTEMNLP05.tff'\n",
    "SL = readSubjectivity(SLpath)\n",
    "\n",
    "featureset3 = [(SL_features(d, word_features, SL), c) for (d, c) in docs]\n",
    "\n",
    "# Cross-validation\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featureset3, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.521"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featureset3 (sentiment lexicon), vocab 1000\n",
    "\n",
    "train_set, test_set = featureset3[1000:], featureset3[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.248      0.196      0.218\n",
      "1 \t      0.258      0.403      0.314\n",
      "2 \t      0.821      0.648      0.724\n",
      "3 \t      0.271      0.431      0.332\n",
      "4 \t      0.252      0.307      0.275\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.370      0.397      0.373\n",
      "\n",
      "Label Counts {0: 455, 1: 1745, 2: 5136, 3: 2026, 4: 638}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.549      0.519      0.521\n",
      "80.67737126350403  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset4 : LIWC, vocab 1000\n",
    "\n",
    "featureset4 = [(liwc_features(d, word_features,poslist,neglist), c) for (d,c) in docs]\n",
    "\n",
    "# Cross-validation\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featureset4, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.538"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featureset4 (LIWC sentiment lexicon), vocab 1000\n",
    "\n",
    "train_set, test_set = featureset4[1000:], featureset4[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.268      0.204      0.231\n",
      "1 \t      0.263      0.411      0.320\n",
      "2 \t      0.816      0.654      0.726\n",
      "3 \t      0.289      0.437      0.347\n",
      "4 \t      0.260      0.308      0.280\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.379      0.403      0.381\n",
      "\n",
      "Label Counts {0: 455, 1: 1745, 2: 5136, 3: 2026, 4: 638}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.552      0.525      0.527\n",
      "82.95425748825073  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset5: Combination SL and LIWC, vocab 1000\n",
    "\n",
    "featureset5 = [(SL_liwc_features(d, word_features, SL, poslist, neglist), c) for (d,c) in docs]\n",
    "\n",
    "# Cross-validation\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featureset5, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.527"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featureset5 (combination SL and LIWC), vocab 1000\n",
    "\n",
    "train_set, test_set = featureset5[1000:], featureset5[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.261      0.153      0.193\n",
      "1 \t      0.251      0.378      0.301\n",
      "2 \t      0.812      0.638      0.714\n",
      "3 \t      0.202      0.392      0.266\n",
      "4 \t      0.209      0.276      0.236\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.347      0.368      0.342\n",
      "\n",
      "Label Counts {0: 455, 1: 1745, 2: 5136, 3: 2026, 4: 638}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.527      0.498      0.497\n",
      "82.21129083633423  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset 6: Part-of-speech tagging, vocab 1000\n",
    "\n",
    "featureset6 = [(POS_features(d, word_features), c) for (d, c) in docs]\n",
    "\n",
    "# Cross-validation\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featureset6, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featureset6 (part-of-speech tagging), vocab 1000\n",
    "\n",
    "train_set, test_set = featureset6[1000:], featureset6[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.441      0.159      0.233\n",
      "1 \t      0.255      0.390      0.308\n",
      "2 \t      0.741      0.677      0.708\n",
      "3 \t      0.238      0.409      0.300\n",
      "4 \t      0.305      0.241      0.268\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.396      0.375      0.363\n",
      "\n",
      "Label Counts {0: 455, 1: 1745, 2: 5136, 3: 2026, 4: 638}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.513      0.521      0.506\n",
      "155.97499585151672  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset 7: Representing negation, vocab 1000\n",
    "\n",
    "featureset7 = [(NOT_features(d, word_features, negationwords), c) for (d, c) in docs]\n",
    "\n",
    "# Cross-validation\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featureset7, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featureset7 (representing negation), vocab 1000\n",
    "\n",
    "train_set, test_set = featureset7[1000:], featureset7[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.272      0.158      0.200\n",
      "1 \t      0.254      0.364      0.299\n",
      "2 \t      0.800      0.635      0.708\n",
      "3 \t      0.195      0.379      0.257\n",
      "4 \t      0.213      0.277      0.240\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.347      0.363      0.341\n",
      "\n",
      "Label Counts {0: 455, 1: 1745, 2: 5136, 3: 2026, 4: 638}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.521      0.492      0.492\n",
      "88.94922804832458  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset 8: Using Bing Liu's Opinion Lexicon, vocab 1000\n",
    "\n",
    "featureset8 = [(liwc_features(d, word_features,poslist2,neglist2), c) for (d,c) in docs]\n",
    "\n",
    "# Cross-validation\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featureset8, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.505"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featureset8 (Bing Liu's Opinion Lexicon), vocab 1000\n",
    "\n",
    "train_set, test_set = featureset8[1000:], featureset8[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10448\n",
      "Each fold size: 1000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.171      0.175      0.173\n",
      "1 \t      0.239      0.357      0.286\n",
      "2 \t      0.817      0.607      0.696\n",
      "3 \t      0.233      0.412      0.297\n",
      "4 \t      0.169      0.244      0.197\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.326      0.359      0.330\n",
      "\n",
      "Label Counts {0: 437, 1: 1794, 2: 4986, 3: 2182, 4: 601}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.519      0.479      0.483\n",
      "60.70177412033081  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset1: Bag of words / unigram (baseline), vocab 500, 10 folds\n",
    "\n",
    "vocab_size = 500\n",
    "limit = int(10000)\n",
    "os.chdir(dirPath)\n",
    "f = open('./train.tsv', 'r')\n",
    "phrasedata=[]\n",
    "for line in f:\n",
    "    if (not line.startswith('Phrase')):\n",
    "        line = line.strip()\n",
    "        phrasedata.append(line.split('\\t')[2:4])\n",
    "random.shuffle(phrasedata)\n",
    "phraselist = phrasedata[:limit]\n",
    "phrasedocs = []\n",
    "for phrase in phraselist:\n",
    "    tokens = nltk.word_tokenize(phrase[0])\n",
    "    phrasedocs.append((tokens, int(phrase[1])))\n",
    "docs = []\n",
    "for phrase in phrasedocs:\n",
    "    lowerphrase = ([w.lower() for w in phrase[0]], phrase[1])\n",
    "    docs.append(lowerphrase)\n",
    "all_words_list = [word for (sent, cat) in docs for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "print(len(all_words))\n",
    "word_items = all_words.most_common(vocab_size)\n",
    "word_features = [word for (word, count) in word_items]\n",
    "featuresets = [(document_features(d, word_features), c) for (d,c) in docs]\n",
    "\n",
    "num_folds = 10\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featuresets, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.522"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featuresets(baseline), vocab 500, 10 folds\n",
    "\n",
    "train_set, test_set = featuresets[1000:], featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10367\n",
      "Each fold size: 1000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.290      0.216      0.247\n",
      "1 \t      0.241      0.386      0.296\n",
      "2 \t      0.817      0.630      0.711\n",
      "3 \t      0.255      0.434      0.321\n",
      "4 \t      0.217      0.301      0.251\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.364      0.393      0.365\n",
      "\n",
      "Label Counts {0: 468, 1: 1743, 2: 5116, 3: 2106, 4: 567}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.540      0.508      0.509\n",
      "127.32099318504333  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset1: Bag of words / unigram (baseline), vocab 1000, 10 folds\n",
    "\n",
    "vocab_size = 1000\n",
    "limit = int(10000)\n",
    "os.chdir(dirPath)\n",
    "f = open('./train.tsv', 'r')\n",
    "phrasedata=[]\n",
    "for line in f:\n",
    "    if (not line.startswith('Phrase')):\n",
    "        line = line.strip()\n",
    "        phrasedata.append(line.split('\\t')[2:4])\n",
    "random.shuffle(phrasedata)\n",
    "phraselist = phrasedata[:limit]\n",
    "phrasedocs = []\n",
    "for phrase in phraselist:\n",
    "    tokens = nltk.word_tokenize(phrase[0])\n",
    "    phrasedocs.append((tokens, int(phrase[1])))\n",
    "docs = []\n",
    "for phrase in phrasedocs:\n",
    "    lowerphrase = ([w.lower() for w in phrase[0]], phrase[1])\n",
    "    docs.append(lowerphrase)\n",
    "all_words_list = [word for (sent, cat) in docs for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "print(len(all_words))\n",
    "word_items = all_words.most_common(vocab_size)\n",
    "word_features = [word for (word, count) in word_items]\n",
    "featuresets = [(document_features(d, word_features), c) for (d,c) in docs]\n",
    "\n",
    "num_folds = 10\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featuresets, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.533"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featuresets(baseline), vocab 1000, 10 folds\n",
    "\n",
    "train_set, test_set = featuresets[1000:], featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10465\n",
      "Each fold size: 1000\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "\n",
      "Average Precision\tRecall\t\tF1 \tPer Label\n",
      "0 \t      0.263      0.227      0.241\n",
      "1 \t      0.228      0.340      0.273\n",
      "2 \t      0.817      0.628      0.710\n",
      "3 \t      0.244      0.418      0.307\n",
      "4 \t      0.237      0.321      0.271\n",
      "\n",
      "Macro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.358      0.387      0.360\n",
      "\n",
      "Label Counts {0: 489, 1: 1718, 2: 5060, 3: 2138, 4: 595}\n",
      "Micro Average Precision\tRecall\t\tF1 \tOver All Labels\n",
      "\t      0.532      0.496      0.500\n",
      "189.41062307357788  seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "# Featureset1: Bag of words / unigram (baseline), vocab 1500, 10 folds\n",
    "\n",
    "vocab_size = 1500\n",
    "limit = int(10000)\n",
    "os.chdir(dirPath)\n",
    "f = open('./train.tsv', 'r')\n",
    "phrasedata=[]\n",
    "for line in f:\n",
    "    if (not line.startswith('Phrase')):\n",
    "        line = line.strip()\n",
    "        phrasedata.append(line.split('\\t')[2:4])\n",
    "random.shuffle(phrasedata)\n",
    "phraselist = phrasedata[:limit]\n",
    "phrasedocs = []\n",
    "for phrase in phraselist:\n",
    "    tokens = nltk.word_tokenize(phrase[0])\n",
    "    phrasedocs.append((tokens, int(phrase[1])))\n",
    "docs = []\n",
    "for phrase in phrasedocs:\n",
    "    lowerphrase = ([w.lower() for w in phrase[0]], phrase[1])\n",
    "    docs.append(lowerphrase)\n",
    "all_words_list = [word for (sent, cat) in docs for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "print(len(all_words))\n",
    "word_items = all_words.most_common(vocab_size)\n",
    "word_features = [word for (word, count) in word_items]\n",
    "featuresets = [(document_features(d, word_features), c) for (d,c) in docs]\n",
    "\n",
    "num_folds = 10\n",
    "start = time.time()\n",
    "cross_validation_PRF(num_folds, featuresets, labels)\n",
    "end = time.time()\n",
    "print(end-start, \" seconds elapsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.561"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes for featuresets(baseline), vocab 1500, 10 folds\n",
    "\n",
    "train_set, test_set = featuresets[1000:], featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
